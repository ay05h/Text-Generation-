{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "input_file = \"my.txt\"\n",
    "punctuation = string.punctuation.replace('.', '').replace('?', '')\n",
    "\n",
    "text_data=\"\"\n",
    "if os.path.exists(input_file) :\n",
    "    with open(input_file, 'r') as infile:\n",
    "        for line in infile:\n",
    "            if line.strip():\n",
    "                for char in punctuation:\n",
    "                    line = line.replace(char, '')\n",
    "                text_data += line\n",
    "\n",
    "print(\"Processed Text:\")\n",
    "print(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "words = nltk.word_tokenize(text_data.lower())\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_int = {w: i for i, w in enumerate(set(words))}\n",
    "int_to_word = {i: w for w, i in word_to_int.items()}\n",
    "len(word_to_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(input_data, seq_length):\n",
    "    sequences = []\n",
    "    for i in range(0, len(input_data) - seq_length):\n",
    "        sequence_in = input_data[i:i + seq_length]\n",
    "        sequence_out = input_data[i + seq_length]\n",
    "        sequences.append((sequence_in, sequence_out))\n",
    "    return sequences\n",
    "\n",
    "encoded_text = np.array([word_to_int[word] for word in words])\n",
    "seq_length = 5\n",
    "sequences = create_sequences(encoded_text, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn1 = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.rnn2 = nn.RNN(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x, hidden1, hidden2):\n",
    "        embedded = self.embedding(x)\n",
    "        out1, hidden1 = self.rnn1(embedded, hidden1)\n",
    "        out2, hidden2 = self.rnn2(out1, hidden2)\n",
    "        out = self.fc(out2[:, -1, :])\n",
    "        return out, hidden1, hidden2\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        return (\n",
    "            torch.zeros(1, batch_size, self.hidden_dim).to(device),\n",
    "            torch.zeros(1, batch_size, self.hidden_dim).to(device),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.955714821815491\n",
      "Epoch 2, Loss: 4.740542113780975\n",
      "Epoch 3, Loss: 4.561698734760284\n",
      "Epoch 4, Loss: 4.388520300388336\n",
      "Epoch 5, Loss: 4.210253298282623\n",
      "Epoch 6, Loss: 4.03250116109848\n",
      "Epoch 7, Loss: 3.841259628534317\n",
      "Epoch 8, Loss: 3.654032737016678\n",
      "Epoch 9, Loss: 3.4553170204162598\n",
      "Epoch 10, Loss: 3.2537152469158173\n",
      "Epoch 11, Loss: 3.062666267156601\n",
      "Epoch 12, Loss: 2.858065515756607\n",
      "Epoch 13, Loss: 2.6713258028030396\n",
      "Epoch 14, Loss: 2.4796840250492096\n",
      "Epoch 15, Loss: 2.2930074632167816\n",
      "Epoch 16, Loss: 2.112635850906372\n",
      "Epoch 17, Loss: 1.9403085857629776\n",
      "Epoch 18, Loss: 1.7764658778905869\n",
      "Epoch 19, Loss: 1.6279776841402054\n",
      "Epoch 20, Loss: 1.4831965863704681\n",
      "Epoch 21, Loss: 1.3467533439397812\n",
      "Epoch 22, Loss: 1.2309203371405602\n",
      "Epoch 23, Loss: 1.1150882691144943\n",
      "Epoch 24, Loss: 1.015949808061123\n",
      "Epoch 25, Loss: 0.918430432677269\n",
      "Epoch 26, Loss: 0.8321503773331642\n",
      "Epoch 27, Loss: 0.7579859718680382\n",
      "Epoch 28, Loss: 0.6903301775455475\n",
      "Epoch 29, Loss: 0.6271351650357246\n",
      "Epoch 30, Loss: 0.5729025527834892\n",
      "Epoch 31, Loss: 0.5228769704699516\n",
      "Epoch 32, Loss: 0.4783107526600361\n",
      "Epoch 33, Loss: 0.4399339631199837\n",
      "Epoch 34, Loss: 0.40465517714619637\n",
      "Epoch 35, Loss: 0.3723475970327854\n",
      "Epoch 36, Loss: 0.345573078840971\n",
      "Epoch 37, Loss: 0.31904811039566994\n",
      "Epoch 38, Loss: 0.2971073351800442\n",
      "Epoch 39, Loss: 0.2766260579228401\n",
      "Epoch 40, Loss: 0.2579187545925379\n",
      "Epoch 41, Loss: 0.24114365689456463\n",
      "Epoch 42, Loss: 0.22639537043869495\n",
      "Epoch 43, Loss: 0.21310545690357685\n",
      "Epoch 44, Loss: 0.20052145048975945\n",
      "Epoch 45, Loss: 0.18842608109116554\n",
      "Epoch 46, Loss: 0.17871214635670185\n",
      "Epoch 47, Loss: 0.16864994540810585\n",
      "Epoch 48, Loss: 0.16012310795485973\n",
      "Epoch 49, Loss: 0.1524349544197321\n",
      "Epoch 50, Loss: 0.14441844448447227\n",
      "Epoch 51, Loss: 0.1378151848912239\n",
      "Epoch 52, Loss: 0.1311190240085125\n",
      "Epoch 53, Loss: 0.12518804986029863\n",
      "Epoch 54, Loss: 0.11985094472765923\n",
      "Epoch 55, Loss: 0.11434441804885864\n",
      "Epoch 56, Loss: 0.1096344105899334\n",
      "Epoch 57, Loss: 0.10500726569443941\n",
      "Epoch 58, Loss: 0.10131874959915876\n",
      "Epoch 59, Loss: 0.09704302623867989\n",
      "Epoch 60, Loss: 0.0936554754152894\n",
      "Epoch 61, Loss: 0.09008030127733946\n",
      "Epoch 62, Loss: 0.08669350761920214\n",
      "Epoch 63, Loss: 0.0837939977645874\n",
      "Epoch 64, Loss: 0.08059655781835318\n",
      "Epoch 65, Loss: 0.07767841313034296\n",
      "Epoch 66, Loss: 0.0752989798784256\n",
      "Epoch 67, Loss: 0.07279187766835093\n",
      "Epoch 68, Loss: 0.0702744210138917\n",
      "Epoch 69, Loss: 0.06808392517268658\n",
      "Epoch 70, Loss: 0.06591933080926538\n",
      "Epoch 71, Loss: 0.06373479124158621\n",
      "Epoch 72, Loss: 0.06190637918189168\n",
      "Epoch 73, Loss: 0.06022108159959316\n",
      "Epoch 74, Loss: 0.05834383750334382\n",
      "Epoch 75, Loss: 0.056833719834685326\n",
      "Epoch 76, Loss: 0.055368888191878796\n",
      "Epoch 77, Loss: 0.05370953306555748\n",
      "Epoch 78, Loss: 0.052220643032342196\n",
      "Epoch 79, Loss: 0.05083516286686063\n",
      "Epoch 80, Loss: 0.04954516468569636\n",
      "Epoch 81, Loss: 0.048212625086307526\n",
      "Epoch 82, Loss: 0.04698733892291784\n",
      "Epoch 83, Loss: 0.045771084260195494\n",
      "Epoch 84, Loss: 0.04463591845706105\n",
      "Epoch 85, Loss: 0.04350935621187091\n",
      "Epoch 86, Loss: 0.042557585053145885\n",
      "Epoch 87, Loss: 0.041570660192519426\n",
      "Epoch 88, Loss: 0.04046047292649746\n",
      "Epoch 89, Loss: 0.03960204217582941\n",
      "Epoch 90, Loss: 0.03870588028803468\n",
      "Epoch 91, Loss: 0.037675369530916214\n",
      "Epoch 92, Loss: 0.03695533936843276\n",
      "Epoch 93, Loss: 0.036151346284896135\n",
      "Epoch 94, Loss: 0.03535933280363679\n",
      "Epoch 95, Loss: 0.034512648824602365\n",
      "Epoch 96, Loss: 0.033877178095281124\n",
      "Epoch 97, Loss: 0.033117618411779404\n",
      "Epoch 98, Loss: 0.03245859034359455\n",
      "Epoch 99, Loss: 0.03172351187095046\n",
      "Epoch 100, Loss: 0.031085103517398238\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_size = len(word_to_int)\n",
    "output_size = len(word_to_int)\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "model = RNNModel(input_size, embedding_dim, hidden_dim, output_size).to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "inputs = np.array([item[0] for item in sequences])\n",
    "targets = np.array([item[1] for item in sequences])\n",
    "\n",
    "inputs_tensor = torch.tensor(inputs, dtype=torch.long)\n",
    "targets_tensor = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(inputs_tensor, targets_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        hidden1, hidden2 = model.init_hidden(inputs.size(0), device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden1, hidden2 = model(inputs, hidden1, hidden2)\n",
    "        loss = loss_function(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {epoch_loss / len(data_loader)}')\n",
    "\n",
    "torch.save(model, 'my_rnn_entire_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence: alice was beginning to get very tired of sitting by\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model = torch.load('my_rnn_entire_model.pth', map_location=torch.device('cpu'))\n",
    "model.eval()\n",
    "\n",
    "def words_to_tensor(words, word_to_int):\n",
    "    indices = [word_to_int[word] for word in words]\n",
    "    tensor = torch.tensor(np.array([indices]), dtype=torch.long)\n",
    "    return tensor\n",
    "\n",
    "def generate_sequence(start_words, model, word_to_int, int_to_word, seq_length=5, length=2):\n",
    "    start_words= start_words.replace('\\n', ' ')\n",
    "    words = nltk.word_tokenize(start_words.lower())  \n",
    "    input_tensor = words_to_tensor(words[-seq_length:], word_to_int)\n",
    "\n",
    "    hidden1, hidden2 = model.init_hidden(batch_size=1, device=torch.device('cpu'))\n",
    "\n",
    "    for _ in range(length):\n",
    "        with torch.no_grad():\n",
    "            output, hidden1, hidden2 = model(input_tensor, hidden1, hidden2)\n",
    "        predicted_index = torch.argmax(output, dim=1).item()\n",
    "        predicted_word = int_to_word[predicted_index]  \n",
    "        words.append(predicted_word)\n",
    "        input_tensor = words_to_tensor(words[-seq_length:], word_to_int)\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "start_words = (\n",
    "    \"Alice was beginning to get very tired of \"\n",
    ")\n",
    "\n",
    "predicted_sequence = generate_sequence(start_words, model, word_to_int, int_to_word)\n",
    "print(f\"Generated sequence: {predicted_sequence}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru1 = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.gru2 = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x, hidden1, hidden2):\n",
    "        embedded = self.embedding(x)\n",
    "        out1, hidden1 = self.gru1(embedded, hidden1)\n",
    "        out2, hidden2 = self.gru2(out1, hidden2)\n",
    "        out = self.fc(out2[:, -1, :])\n",
    "        return out, hidden1, hidden2\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        return (\n",
    "            torch.zeros(1, batch_size, self.hidden_dim).to(device),\n",
    "            torch.zeros(1, batch_size, self.hidden_dim).to(device),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.945730566978455\n",
      "Epoch 2, Loss: 4.872101783752441\n",
      "Epoch 3, Loss: 4.811259031295776\n",
      "Epoch 4, Loss: 4.75011020898819\n",
      "Epoch 5, Loss: 4.684012532234192\n",
      "Epoch 6, Loss: 4.611028492450714\n",
      "Epoch 7, Loss: 4.52817839384079\n",
      "Epoch 8, Loss: 4.432860374450684\n",
      "Epoch 9, Loss: 4.312912046909332\n",
      "Epoch 10, Loss: 4.1786646246910095\n",
      "Epoch 11, Loss: 4.033781945705414\n",
      "Epoch 12, Loss: 3.8974643647670746\n",
      "Epoch 13, Loss: 3.762934535741806\n",
      "Epoch 14, Loss: 3.6094580590724945\n",
      "Epoch 15, Loss: 3.450372636318207\n",
      "Epoch 16, Loss: 3.284603625535965\n",
      "Epoch 17, Loss: 3.1213663816452026\n",
      "Epoch 18, Loss: 2.9479028284549713\n",
      "Epoch 19, Loss: 2.782639741897583\n",
      "Epoch 20, Loss: 2.6215376555919647\n",
      "Epoch 21, Loss: 2.4575088024139404\n",
      "Epoch 22, Loss: 2.2981907725334167\n",
      "Epoch 23, Loss: 2.1519567370414734\n",
      "Epoch 24, Loss: 1.9998178631067276\n",
      "Epoch 25, Loss: 1.8665757775306702\n",
      "Epoch 26, Loss: 1.729377418756485\n",
      "Epoch 27, Loss: 1.6064624786376953\n",
      "Epoch 28, Loss: 1.487868919968605\n",
      "Epoch 29, Loss: 1.376137763261795\n",
      "Epoch 30, Loss: 1.2690048664808273\n",
      "Epoch 31, Loss: 1.180613949894905\n",
      "Epoch 32, Loss: 1.0911087095737457\n",
      "Epoch 33, Loss: 1.0013493746519089\n",
      "Epoch 34, Loss: 0.9279307425022125\n",
      "Epoch 35, Loss: 0.85809076577425\n",
      "Epoch 36, Loss: 0.7929891124367714\n",
      "Epoch 37, Loss: 0.7346524745225906\n",
      "Epoch 38, Loss: 0.6798224002122879\n",
      "Epoch 39, Loss: 0.6307980641722679\n",
      "Epoch 40, Loss: 0.5844343677163124\n",
      "Epoch 41, Loss: 0.544273242354393\n",
      "Epoch 42, Loss: 0.5058198682963848\n",
      "Epoch 43, Loss: 0.47223803400993347\n",
      "Epoch 44, Loss: 0.440323356539011\n",
      "Epoch 45, Loss: 0.4116256535053253\n",
      "Epoch 46, Loss: 0.38633354753255844\n",
      "Epoch 47, Loss: 0.3619724363088608\n",
      "Epoch 48, Loss: 0.34002313762903214\n",
      "Epoch 49, Loss: 0.32020512223243713\n",
      "Epoch 50, Loss: 0.3013886734843254\n",
      "Epoch 51, Loss: 0.2852363549172878\n",
      "Epoch 52, Loss: 0.26932046562433243\n",
      "Epoch 53, Loss: 0.2548282127827406\n",
      "Epoch 54, Loss: 0.24057026207447052\n",
      "Epoch 55, Loss: 0.22882864996790886\n",
      "Epoch 56, Loss: 0.2174242716282606\n",
      "Epoch 57, Loss: 0.20612805895507336\n",
      "Epoch 58, Loss: 0.19629350677132607\n",
      "Epoch 59, Loss: 0.1873689703643322\n",
      "Epoch 60, Loss: 0.1786410305649042\n",
      "Epoch 61, Loss: 0.1709465403109789\n",
      "Epoch 62, Loss: 0.1626469474285841\n",
      "Epoch 63, Loss: 0.15630445070564747\n",
      "Epoch 64, Loss: 0.150054432451725\n",
      "Epoch 65, Loss: 0.14357057958841324\n",
      "Epoch 66, Loss: 0.13743531797081232\n",
      "Epoch 67, Loss: 0.13235255889594555\n",
      "Epoch 68, Loss: 0.12738052289932966\n",
      "Epoch 69, Loss: 0.12214243412017822\n",
      "Epoch 70, Loss: 0.11794205009937286\n",
      "Epoch 71, Loss: 0.11364089883863926\n",
      "Epoch 72, Loss: 0.1093744533136487\n",
      "Epoch 73, Loss: 0.10576837882399559\n",
      "Epoch 74, Loss: 0.10168631561100483\n",
      "Epoch 75, Loss: 0.09843698889017105\n",
      "Epoch 76, Loss: 0.09497192781418562\n",
      "Epoch 77, Loss: 0.09176854602992535\n",
      "Epoch 78, Loss: 0.08862500917166471\n",
      "Epoch 79, Loss: 0.0862019918859005\n",
      "Epoch 80, Loss: 0.08313993271440268\n",
      "Epoch 81, Loss: 0.0807524174451828\n",
      "Epoch 82, Loss: 0.07837169989943504\n",
      "Epoch 83, Loss: 0.07625877019017935\n",
      "Epoch 84, Loss: 0.07361874915659428\n",
      "Epoch 85, Loss: 0.07154879812151194\n",
      "Epoch 86, Loss: 0.06956308335065842\n",
      "Epoch 87, Loss: 0.06776055879890919\n",
      "Epoch 88, Loss: 0.06586563866585493\n",
      "Epoch 89, Loss: 0.0638069361448288\n",
      "Epoch 90, Loss: 0.06224188441410661\n",
      "Epoch 91, Loss: 0.060540216974914074\n",
      "Epoch 92, Loss: 0.05891455290839076\n",
      "Epoch 93, Loss: 0.057308773044496775\n",
      "Epoch 94, Loss: 0.05581576004624367\n",
      "Epoch 95, Loss: 0.05456120613962412\n",
      "Epoch 96, Loss: 0.05319672590121627\n",
      "Epoch 97, Loss: 0.0519499983638525\n",
      "Epoch 98, Loss: 0.05066806497052312\n",
      "Epoch 99, Loss: 0.049502397421747446\n",
      "Epoch 100, Loss: 0.04825412202626467\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_size = len(word_to_int)\n",
    "output_size = len(word_to_int)\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "model = GRUModel(input_size, embedding_dim, hidden_dim, output_size).to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "inputs = np.array([item[0] for item in sequences])\n",
    "targets = np.array([item[1] for item in sequences])\n",
    "\n",
    "inputs_tensor = torch.tensor(inputs, dtype=torch.long)\n",
    "targets_tensor = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(inputs_tensor, targets_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        hidden1, hidden2 = model.init_hidden(inputs.size(0), device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden1, hidden2 = model(inputs, hidden1, hidden2)\n",
    "        loss = loss_function(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {epoch_loss / len(data_loader)}')\n",
    "torch.save(model, 'my_gru_entire_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence: there was nothing so very remarkable in that\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model = torch.load('my_gru_entire_model.pth', map_location=torch.device('cpu'))\n",
    "model.eval()\n",
    "\n",
    "def words_to_tensor(words, word_to_int):\n",
    "    indices = [word_to_int[word] for word in words]\n",
    "    tensor = torch.tensor(np.array([indices]), dtype=torch.long)\n",
    "    return tensor\n",
    "\n",
    "def generate_sequence(start_words, model, word_to_int, int_to_word, seq_length=5, length=2):\n",
    "    start_words= start_words.replace('\\n', ' ')\n",
    "    words = nltk.word_tokenize(start_words.lower())  \n",
    "    input_tensor = words_to_tensor(words[-seq_length:], word_to_int)\n",
    "\n",
    "    hidden1, hidden2 = model.init_hidden(batch_size=1, device=torch.device('cpu'))\n",
    "\n",
    "    for _ in range(length):\n",
    "        with torch.no_grad():\n",
    "            output, hidden1, hidden2 = model(input_tensor, hidden1, hidden2)\n",
    "        predicted_index = torch.argmax(output, dim=1).item()\n",
    "        predicted_word = int_to_word[predicted_index]  \n",
    "        words.append(predicted_word)\n",
    "        input_tensor = words_to_tensor(words[-seq_length:], word_to_int)\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "start_words = (\n",
    "    \"There was nothing so VERY remarkable\"\n",
    ")\n",
    "\n",
    "predicted_sequence = generate_sequence(start_words, model, word_to_int, int_to_word)\n",
    "print(f\"Generated sequence: {predicted_sequence}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x, hidden1, hidden2):\n",
    "        embedded = self.embedding(x)\n",
    "        out1, hidden1 = self.lstm1(embedded, hidden1)\n",
    "        out2, hidden2 = self.lstm2(out1, hidden2)\n",
    "        out = self.fc(out2[:, -1, :])\n",
    "        return out, hidden1, hidden2\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        return (\n",
    "            (torch.zeros(1, batch_size, self.hidden_dim).to(device),  \n",
    "             torch.zeros(1, batch_size, self.hidden_dim).to(device)),  \n",
    "            (torch.zeros(1, batch_size, self.hidden_dim).to(device),  \n",
    "             torch.zeros(1, batch_size, self.hidden_dim).to(device))   \n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.942196428775787\n",
      "Epoch 2, Loss: 4.920147776603699\n",
      "Epoch 3, Loss: 4.9016329646110535\n",
      "Epoch 4, Loss: 4.881509244441986\n",
      "Epoch 5, Loss: 4.859233915805817\n",
      "Epoch 6, Loss: 4.8325188755989075\n",
      "Epoch 7, Loss: 4.796272099018097\n",
      "Epoch 8, Loss: 4.742726266384125\n",
      "Epoch 9, Loss: 4.658486843109131\n",
      "Epoch 10, Loss: 4.501550137996674\n",
      "Epoch 11, Loss: 4.3161925077438354\n",
      "Epoch 12, Loss: 4.178343266248703\n",
      "Epoch 13, Loss: 3.9984431862831116\n",
      "Epoch 14, Loss: 3.7966540157794952\n",
      "Epoch 15, Loss: 3.5781111419200897\n",
      "Epoch 16, Loss: 3.3328101336956024\n",
      "Epoch 17, Loss: 3.0670833587646484\n",
      "Epoch 18, Loss: 2.809463530778885\n",
      "Epoch 19, Loss: 2.5588470697402954\n",
      "Epoch 20, Loss: 2.3401624858379364\n",
      "Epoch 21, Loss: 2.1110410690307617\n",
      "Epoch 22, Loss: 1.925069972872734\n",
      "Epoch 23, Loss: 1.7396045923233032\n",
      "Epoch 24, Loss: 1.5761377811431885\n",
      "Epoch 25, Loss: 1.4336453676223755\n",
      "Epoch 26, Loss: 1.295258805155754\n",
      "Epoch 27, Loss: 1.182697519659996\n",
      "Epoch 28, Loss: 1.0772036463022232\n",
      "Epoch 29, Loss: 0.9848990812897682\n",
      "Epoch 30, Loss: 0.8966527506709099\n",
      "Epoch 31, Loss: 0.8256253898143768\n",
      "Epoch 32, Loss: 0.7577050030231476\n",
      "Epoch 33, Loss: 0.6965640559792519\n",
      "Epoch 34, Loss: 0.6410901620984077\n",
      "Epoch 35, Loss: 0.590758066624403\n",
      "Epoch 36, Loss: 0.5462770760059357\n",
      "Epoch 37, Loss: 0.5046958401799202\n",
      "Epoch 38, Loss: 0.46845842897892\n",
      "Epoch 39, Loss: 0.4362139068543911\n",
      "Epoch 40, Loss: 0.40744804963469505\n",
      "Epoch 41, Loss: 0.37873350456357\n",
      "Epoch 42, Loss: 0.35488202050328255\n",
      "Epoch 43, Loss: 0.3340817466378212\n",
      "Epoch 44, Loss: 0.3130723498761654\n",
      "Epoch 45, Loss: 0.29489027708768845\n",
      "Epoch 46, Loss: 0.27671439200639725\n",
      "Epoch 47, Loss: 0.2640919219702482\n",
      "Epoch 48, Loss: 0.248191487044096\n",
      "Epoch 49, Loss: 0.23459938913583755\n",
      "Epoch 50, Loss: 0.22120101936161518\n",
      "Epoch 51, Loss: 0.21133099123835564\n",
      "Epoch 52, Loss: 0.20073292218148708\n",
      "Epoch 53, Loss: 0.19045818038284779\n",
      "Epoch 54, Loss: 0.18185285106301308\n",
      "Epoch 55, Loss: 0.17418359406292439\n",
      "Epoch 56, Loss: 0.16548134945333004\n",
      "Epoch 57, Loss: 0.15831451676785946\n",
      "Epoch 58, Loss: 0.151580473408103\n",
      "Epoch 59, Loss: 0.14476647414267063\n",
      "Epoch 60, Loss: 0.13822879642248154\n",
      "Epoch 61, Loss: 0.1334452172741294\n",
      "Epoch 62, Loss: 0.1276763565838337\n",
      "Epoch 63, Loss: 0.12233538273721933\n",
      "Epoch 64, Loss: 0.11835560202598572\n",
      "Epoch 65, Loss: 0.11384348198771477\n",
      "Epoch 66, Loss: 0.10965185891836882\n",
      "Epoch 67, Loss: 0.10576350148767233\n",
      "Epoch 68, Loss: 0.10194267146289349\n",
      "Epoch 69, Loss: 0.0980090145021677\n",
      "Epoch 70, Loss: 0.09478749893605709\n",
      "Epoch 71, Loss: 0.09164565335959196\n",
      "Epoch 72, Loss: 0.08862666226923466\n",
      "Epoch 73, Loss: 0.08586089313030243\n",
      "Epoch 74, Loss: 0.08333337306976318\n",
      "Epoch 75, Loss: 0.08046608604490757\n",
      "Epoch 76, Loss: 0.07780657522380352\n",
      "Epoch 77, Loss: 0.07557463645935059\n",
      "Epoch 78, Loss: 0.07342831417918205\n",
      "Epoch 79, Loss: 0.0711261359974742\n",
      "Epoch 80, Loss: 0.06905703712254763\n",
      "Epoch 81, Loss: 0.06701167533174157\n",
      "Epoch 82, Loss: 0.0651570437476039\n",
      "Epoch 83, Loss: 0.06358600687235594\n",
      "Epoch 84, Loss: 0.06140583520755172\n",
      "Epoch 85, Loss: 0.06004173820838332\n",
      "Epoch 86, Loss: 0.058373632840812206\n",
      "Epoch 87, Loss: 0.05699277762323618\n",
      "Epoch 88, Loss: 0.0555241871625185\n",
      "Epoch 89, Loss: 0.05406060582026839\n",
      "Epoch 90, Loss: 0.05264992080628872\n",
      "Epoch 91, Loss: 0.05144317401573062\n",
      "Epoch 92, Loss: 0.050131408497691154\n",
      "Epoch 93, Loss: 0.04908500798046589\n",
      "Epoch 94, Loss: 0.047898832242935896\n",
      "Epoch 95, Loss: 0.0467816898599267\n",
      "Epoch 96, Loss: 0.04567701043561101\n",
      "Epoch 97, Loss: 0.044719874393194914\n",
      "Epoch 98, Loss: 0.04364520078524947\n",
      "Epoch 99, Loss: 0.04274749383330345\n",
      "Epoch 100, Loss: 0.04166284902021289\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_size = len(word_to_int)\n",
    "output_size = len(word_to_int)\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "model = LSTMModel(input_size, embedding_dim, hidden_dim, output_size).to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "inputs = np.array([item[0] for item in sequences])\n",
    "targets = np.array([item[1] for item in sequences])\n",
    "\n",
    "inputs_tensor = torch.tensor(inputs, dtype=torch.long)\n",
    "targets_tensor = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(inputs_tensor, targets_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        hidden1, hidden2 = model.init_hidden(inputs.size(0), device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden1, hidden2 = model(inputs, hidden1, hidden2)\n",
    "\n",
    "        loss = loss_function(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {epoch_loss / len(data_loader)}')\n",
    "torch.save(model, 'my_lstm_entire_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence: alice was beginning to get very tired of sitting\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model = torch.load('my_lstm_entire_model.pth', map_location=torch.device('cpu'))\n",
    "model.eval()\n",
    "\n",
    "def words_to_tensor(words, word_to_int):\n",
    "    indices = [word_to_int[word] for word in words]\n",
    "    tensor = torch.tensor(np.array([indices]), dtype=torch.long)\n",
    "    return tensor\n",
    "\n",
    "def generate_sequence(start_words, model, word_to_int, int_to_word, seq_length=5, length=1):\n",
    "    start_words= start_words.replace('\\n', ' ')\n",
    "    words = nltk.word_tokenize(start_words.lower())  \n",
    "    input_tensor = words_to_tensor(words[-seq_length:], word_to_int)\n",
    "\n",
    "    hidden1, hidden2 = model.init_hidden(batch_size=1, device=torch.device('cpu'))\n",
    "\n",
    "    for _ in range(length):\n",
    "        with torch.no_grad():\n",
    "            output, hidden1, hidden2 = model(input_tensor, hidden1, hidden2)\n",
    "        predicted_index = torch.argmax(output, dim=1).item()\n",
    "        predicted_word = int_to_word[predicted_index]  \n",
    "        words.append(predicted_word)\n",
    "        input_tensor = words_to_tensor(words[-seq_length:], word_to_int)\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "start_words = (\n",
    "    \"Alice was beginning to get very tired of\"\n",
    ")\n",
    "\n",
    "predicted_sequence = generate_sequence(start_words, model, word_to_int, int_to_word)\n",
    "print(f\"Generated sequence: {predicted_sequence}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.955663621425629\n",
      "Epoch 2, Loss: 4.899330914020538\n",
      "Epoch 3, Loss: 4.850935995578766\n",
      "Epoch 4, Loss: 4.805143237113953\n",
      "Epoch 5, Loss: 4.757228434085846\n",
      "Epoch 6, Loss: 4.7090007066726685\n",
      "Epoch 7, Loss: 4.657526910305023\n",
      "Epoch 8, Loss: 4.604125559329987\n",
      "Epoch 9, Loss: 4.545733392238617\n",
      "Epoch 10, Loss: 4.487375795841217\n",
      "Epoch 11, Loss: 4.420965135097504\n",
      "Epoch 12, Loss: 4.347955584526062\n",
      "Epoch 13, Loss: 4.2679367661476135\n",
      "Epoch 14, Loss: 4.1809640526771545\n",
      "Epoch 15, Loss: 4.084452688694\n",
      "Epoch 16, Loss: 3.9825932383537292\n",
      "Epoch 17, Loss: 3.8717805445194244\n",
      "Epoch 18, Loss: 3.7453378438949585\n",
      "Epoch 19, Loss: 3.6159085035324097\n",
      "Epoch 20, Loss: 3.482541650533676\n",
      "Epoch 21, Loss: 3.3342038691043854\n",
      "Epoch 22, Loss: 3.1909786760807037\n",
      "Epoch 23, Loss: 3.043420672416687\n",
      "Epoch 24, Loss: 2.8992905616760254\n",
      "Epoch 25, Loss: 2.744039237499237\n",
      "Epoch 26, Loss: 2.593690037727356\n",
      "Epoch 27, Loss: 2.4527108669281006\n",
      "Epoch 28, Loss: 2.303657203912735\n",
      "Epoch 29, Loss: 2.162389785051346\n",
      "Epoch 30, Loss: 2.031339630484581\n",
      "Epoch 31, Loss: 1.8991422355175018\n",
      "Epoch 32, Loss: 1.770573541522026\n",
      "Epoch 33, Loss: 1.6475875973701477\n",
      "Epoch 34, Loss: 1.5284022092819214\n",
      "Epoch 35, Loss: 1.4101732820272446\n",
      "Epoch 36, Loss: 1.3090757429599762\n",
      "Epoch 37, Loss: 1.2080328166484833\n",
      "Epoch 38, Loss: 1.1132646352052689\n",
      "Epoch 39, Loss: 1.0239157602190971\n",
      "Epoch 40, Loss: 0.939890168607235\n",
      "Epoch 41, Loss: 0.8593998402357101\n",
      "Epoch 42, Loss: 0.7909774854779243\n",
      "Epoch 43, Loss: 0.726373016834259\n",
      "Epoch 44, Loss: 0.6637916415929794\n",
      "Epoch 45, Loss: 0.6107900962233543\n",
      "Epoch 46, Loss: 0.5608229413628578\n",
      "Epoch 47, Loss: 0.5134398154914379\n",
      "Epoch 48, Loss: 0.47109492123126984\n",
      "Epoch 49, Loss: 0.4339649975299835\n",
      "Epoch 50, Loss: 0.39999525621533394\n",
      "Epoch 51, Loss: 0.3695012107491493\n",
      "Epoch 52, Loss: 0.34050093591213226\n",
      "Epoch 53, Loss: 0.3163086622953415\n",
      "Epoch 54, Loss: 0.29317687451839447\n",
      "Epoch 55, Loss: 0.2739412672817707\n",
      "Epoch 56, Loss: 0.25476562045514584\n",
      "Epoch 57, Loss: 0.23906981386244297\n",
      "Epoch 58, Loss: 0.22293471544981003\n",
      "Epoch 59, Loss: 0.20858061499893665\n",
      "Epoch 60, Loss: 0.19522548839449883\n",
      "Epoch 61, Loss: 0.18363161198794842\n",
      "Epoch 62, Loss: 0.1731913834810257\n",
      "Epoch 63, Loss: 0.16385352052748203\n",
      "Epoch 64, Loss: 0.15440464578568935\n",
      "Epoch 65, Loss: 0.14684668369591236\n",
      "Epoch 66, Loss: 0.13909529708325863\n",
      "Epoch 67, Loss: 0.13203588128089905\n",
      "Epoch 68, Loss: 0.12550625950098038\n",
      "Epoch 69, Loss: 0.11971095949411392\n",
      "Epoch 70, Loss: 0.11352725233882666\n",
      "Epoch 71, Loss: 0.10859575029462576\n",
      "Epoch 72, Loss: 0.10396176110953093\n",
      "Epoch 73, Loss: 0.09950591344386339\n",
      "Epoch 74, Loss: 0.09506538696587086\n",
      "Epoch 75, Loss: 0.09111197385936975\n",
      "Epoch 76, Loss: 0.08741537760943174\n",
      "Epoch 77, Loss: 0.08418905269354582\n",
      "Epoch 78, Loss: 0.08087423350661993\n",
      "Epoch 79, Loss: 0.0778131727129221\n",
      "Epoch 80, Loss: 0.07497845869511366\n",
      "Epoch 81, Loss: 0.07216874044388533\n",
      "Epoch 82, Loss: 0.06948763504624367\n",
      "Epoch 83, Loss: 0.06701121665537357\n",
      "Epoch 84, Loss: 0.06480215908959508\n",
      "Epoch 85, Loss: 0.06252325186505914\n",
      "Epoch 86, Loss: 0.06050669448450208\n",
      "Epoch 87, Loss: 0.058479472529143095\n",
      "Epoch 88, Loss: 0.05657631019130349\n",
      "Epoch 89, Loss: 0.05475321365520358\n",
      "Epoch 90, Loss: 0.052825762424618006\n",
      "Epoch 91, Loss: 0.051417308393865824\n",
      "Epoch 92, Loss: 0.04990845965221524\n",
      "Epoch 93, Loss: 0.04823110345751047\n",
      "Epoch 94, Loss: 0.04697890765964985\n",
      "Epoch 95, Loss: 0.045554171316325665\n",
      "Epoch 96, Loss: 0.04431554675102234\n",
      "Epoch 97, Loss: 0.042946763802319765\n",
      "Epoch 98, Loss: 0.041781733743846416\n",
      "Epoch 99, Loss: 0.04065698618069291\n",
      "Epoch 100, Loss: 0.0396250388585031\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class SimpleTransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_heads, seq_length):\n",
    "        super(SimpleTransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, seq_length, embedding_dim))\n",
    "        self.attention = nn.MultiheadAttention(embedding_dim, num_heads, batch_first=True)\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.positional_encoding[:, :x.size(1), :]\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        logits = self.fc(attn_output[:, -1, :])  \n",
    "        return logits\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "vocab_size = len(word_to_int)  \n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "num_heads = 4\n",
    "seq_length = 5\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "learning_rate = 0.0001\n",
    "\n",
    "model = SimpleTransformerModel(vocab_size, embedding_dim, hidden_dim, num_heads, seq_length).to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "inputs = np.array([item[0] for item in sequences])\n",
    "targets = np.array([item[1] for item in sequences])\n",
    "\n",
    "inputs_tensor = torch.tensor(inputs, dtype=torch.long)\n",
    "targets_tensor = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(inputs_tensor, targets_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = loss_function(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {epoch_loss / len(data_loader)}')\n",
    "\n",
    "# Save the entire model\n",
    "torch.save(model, 'my_transformer_entire_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence: alice was beginning to get very tired of sitting by\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model = torch.load('my_transformer_entire_model.pth', map_location=torch.device('cpu'))\n",
    "model.eval()\n",
    "\n",
    "def words_to_tensor(words, word_to_int):\n",
    "    indices = [word_to_int[word] for word in words]\n",
    "    tensor = torch.tensor(np.array([indices]), dtype=torch.long)\n",
    "    return tensor\n",
    "\n",
    "def generate_sequence(start_words, model, word_to_int, int_to_word, seq_length=5, length=2):\n",
    "    start_words= start_words.replace('\\n', ' ')\n",
    "    words = nltk.word_tokenize(start_words.lower())  \n",
    "    input_tensor = words_to_tensor(words[-seq_length:], word_to_int)\n",
    " \n",
    "    for _ in range(length):\n",
    "        with torch.no_grad():\n",
    "            output= model(input_tensor)\n",
    "        predicted_index = torch.argmax(output, dim=1).item()\n",
    "        predicted_word = int_to_word[predicted_index]  \n",
    "        words.append(predicted_word)\n",
    "        input_tensor = words_to_tensor(words[-seq_length:], word_to_int)\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "start_words = (\n",
    "    \"Alice was beginning to get very tired of\"\n",
    ")\n",
    "\n",
    "predicted_sequence = generate_sequence(start_words, model, word_to_int, int_to_word)\n",
    "print(f\"Generated sequence: {predicted_sequence}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
